<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>

<head>
     <title>papers</title>
</head>

<body>

     <!-- This document was automatically generated with bibtex2html 1.99
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html -q papers.bib  -->


     <table>

          <tr valign="top">
               <td align="right" class="bibtexnumber">
                    [<a name="pmlr-v139-brooks21a">1</a>]
               </td>
               <td class="bibtexitem">
                    <b>Brooks, Ethan</b>, Janarthanan Rajendran, Richard&nbsp;L Lewis, and Satinder
                    Singh.
                    Reinforcement learning of implicit and explicit control flow
                    instructions.
                    In Marina Meila and Tong Zhang, editors, <em>Proceedings of the 38th
                         International Conference on Machine Learning</em>, volume 139 of <em>Proceedings
                         of Machine Learning Research</em>, pages 1082--1091. PMLR, 18--24 Jul 2021.
                    [&nbsp;<a href="papers_bib.html#pmlr-v139-brooks21a">bib</a>&nbsp;|
                    <a href="https://proceedings.mlr.press/v139/brooks21a.html">.html</a>&nbsp;|
                    <a href="http://proceedings.mlr.press/v139/brooks21a/brooks21a.pdf">.pdf</a>&nbsp;]
                    <blockquote>
                         <font size="-1">
                              Learning to flexibly follow task instructions in dynamic environments poses interesting
                              challenges for reinforcement learning agents. We focus here on the problem of learning
                              control flow that deviates from a strict step-by-step execution of instructions—that is,
                              control flow that may skip forward over parts of the instructions or return backward to
                              previously completed or skipped steps. Demand for such flexible control arises in two
                              fundamental ways: explicitly when control is specified in the instructions themselves
                              (such as conditional branching and looping) and implicitly when stochastic environment
                              dynamics require re-completion of instructions whose effects have been perturbed, or
                              opportunistic skipping of instructions whose effects are already present. We formulate an
                              attention-based architecture that meets these challenges by learning, from task reward
                              only, to flexibly attend to and condition behavior on an internal encoding of the
                              instructions. We test the architecture’s ability to learn both explicit and implicit
                              control in two illustrative domains—one inspired by Minecraft and the other by
                              StarCraft—and show that the architecture exhibits zero-shot generalization to novel
                              instructions of length greater than those in a training set, at a performance level
                              unmatched by three baseline recurrent architectures and one ablation architecture.
                         </font>
                    </blockquote>

               </td>
          </tr>


          <tr valign="top">
               <td align="right" class="bibtexnumber">
                    [<a name="1stlt2014hazing">2</a>]
               </td>
               <td class="bibtexitem">
                    Ethan Brooks.
                    Hazing versus challenging.
                    <em>Marine Corps Gazette</em>, 98(8):24--25, 2014.
                    [&nbsp;<a href="papers_bib.html#1stlt2014hazing">bib</a>&nbsp;]

               </td>
          </tr>
     </table>
     <hr>
     <p><em>This file was generated by
               <a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.99.</em></p>
</body>

</html>
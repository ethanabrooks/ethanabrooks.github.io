<p>
    I am a machine learning researcher in the subdiscipline of deep
    reinforcement learning. I am interested in using foundation models to speed
    up learning. Until recently, the dominant trend in machine learning has been
    a move away from priors and inductive biases toward general methods that
    make minimal assumptions about the problem domain. The cost of this approach
    has been a massive increase in the quantity of data required for learning.
    Recent innovations in meta-learning have demonstrated some success at
    addressing this issue, but many of these methods have not been shown to
    scale. I believe that the time is right for a return to priors and inductive
    biases. Unlike the past, we must now focus on learned priors and biases
    distilled from the empirical world by our foundation models. A method that
    works in any world will be slow. A method that only works in
    this world will be fast!
</p>
---
layout: post
title: Machines That Manipulate State
date: 2016-11-03
---
If someone claimed that modern machines were intelligent, how would we disprove it? We could think of tasks that humans can perform that machines cannot, but this is a two-way street: in some tasks, machines vastly outperform humans. Besides, examples alone can never get to the underlying principles. We could use terms like "general" intelligence or "reason", but what exactly do these terms mean?

To be clear, my interest is not merely philosophical. I am interested in _mechanisms_ that make complex cognition in the absence of structure possible. I believe that developments in machine learning bring us closer to answering these questions. Specifically, there is a development common to many recent innovations that I think touches on some of the most essential elements of intelligence: in contrast to traditional machine learning models, which take in data and output predictions, many of the most successful recent models incorporate mechanisms that take in _**state**_ and output _**state**_. This is actually the defining characteristic of a deep network: inner layers of a network operate on state, not directly on data or predictions. I think that a model's ability to manipulate, mutate, and store state is one of the main determiners of intelligence. Discussion of a specific example will hopefully strengthen this claim:

When I ride a bicycle, my brain seems to upload what we might call a bicycle-riding "program" from memory. This "uploading" mechanism seems to take cues not from external sensory inputs but from some kind of internal signal (maybe something we could call _volition_?). Additionally, my brain draws on complex memories, such as the meaning of traffic signs, that are relevant to the activity. It also attends carefully to certain sensory signals like the movements of cars, while ignoring other signals that might be important in other circumstances, like expressions on faces. Given this complex system of conditioning mechanisms, my behavior on the bicycle from moment to moment requires little attention or effort.

In general, it seems to be true that in the moment of activity while performing a certain task, behavior is often (always?) automatic. Indeed, the progress of machine learning research suggests that current architectures are capable of mastering many moment-to-moment inference tasks. What distinguishes human intelligence is _not_ the specific architecture for performing inference, but the _supporting_ architectures that set the stage for inference. In computational terms, the significant element is not the CPU, but the programs that populate the computer memory and establish the state of the system before the computation is even performed.

The power of this meta capability becomes evident in applications that require rapid learning, for example, one-shot learning. In these applications, a model does not have time to slowly learn the inference function using gradient descent. Instead, models have to learn a more abstract mapping: from inputs to hidden states that then facilitate the rapid learning of the actual function. For example, when a person learns a new word, the brain's rapid assimilation of the word into vocabulary clearly suggests the presence of more a powerful _learned_ program specifically responsible for vocabulary acquisition.

How are modern deep learning architectures manifesting these capabilities? LSTMs and memory networks learn to store state in mutable in memory systems. In reinforcement learning, we use a neural network to infer the current state of the system. Unlike ordinary learning by gradient descent, which is slow and stops once training is complete, these more advanced networks perform mutations on hidden state even after training is complete without receiving feedback.

What happens when this idea is taken to its logical extreme? LSTMs operate on their own parameters, but imagine a model that operates on programs themselves. Humans seem to exhibit this capability: most ordinary adults with a sense of balance and a basic familiarity with wheeled vehicles can almost immediately learn to ride a scooter. Somehow, the mind identifies related, learned programs and combines them to create new ones. Though slow, Hebbian learning might gradually improve the abilities of the scooter rider, the initial encounter with the scooter only benefits from learning related to other tasks. For most tasks, _manipulation and recombination of existing programs seems to account for the bulk of overall learning_.

<!-- How are modern deep learning architectures manifesting these capabilities? It is worth noting that the kind of state mutation that I am interested in is distinct from the mutation that occurs through normal training. Learning by gradient descent is slow and stops once training is complete. In contrast, a more advanced and expressive network like an LSTM can perform mutations on its hidden state even after training is complete, without receiving feedback. -->

<!-- Maybe something to do with emergent behavior, activity in the absence of structure, the ability to identify intermediate objectives in the pursuit of distant ones. -->


<!-- My research interests are based on this premise: the mental architecture that supports generalized, human-like intelligence is not very different in kind from a recurrent neural network. The distinguishing characteristic of the human mind is its ability to dramatically alter its _state_ to suit whatever task the circumstances call for.

In the moment of inference, the architecture of the brain performs a feedforward pass through a neural network. What distinguishes the brain from any other neural model is not its activity at this moment, but the _context_ in which it performs the action. In other words, unlike a standard neural network, the ... -->
